<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet" />
    <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
    <title>Michael Adjeisah's Homepage</title>
    <link rel="icon" type="image/png" href="images/icon.png">
    <!-- <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
Reflux Template
https://templatemo.com/tm-531-reflux
-->
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-style.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/lightbox.css" />
    <link rel="stylesheet" href="assets/css/academicons.min.css" />
</head>

<body>
    <div id="page-wraper">
        <!-- Sidebar Menu -->
        <div class="responsive-nav">
            <i class="fa fa-bars" id="menu-toggle"></i>
            <div id="menu" class="menu">
                <i class="fa fa-times" id="menu-close"></i>
                <div class="container">
                    <div class="image">
                        <a href="#"><img src="assets/images/micky_.png" alt="" /></a>
                    </div>
                    <div class="author-content">
                        <h4>Michael Adjeisah</h4>
                        <p class="author__bio">Postdoctoral Research Fellow, <br> ZJNU </p>
                    </div>
                    <nav class="main-nav" role="navigation">
                        <ul class="main-menu">
                            <li><a href="#section1">About Me</a></li>
                            <!-- <li><a href="#section2">My Research</a></li> --> 
                            <li><a href="#section3">Publications</a></li>
                            <li><a href="#section4">Courses</a></li>
                            <li><a href="#section5">Academic Activities</a></li>
                        </ul>
                    </nav>
                    <div class="social-network">
                        <ul class="soial-icons">
                            <li>
                                <a href="https://scholar.google.com/citations?user=3W3g6YkAAAAJ&hl=en&oi=sra"><i class="ai ai-google-scholar-square"></i></a>
                            </li>
                            <li>
                                <a href="https://github.com/Madjeisah"><i class="fa-brands fa-github"></i></a>
                            </li>
                            <li>
                                <a href="files/Resume_MichaelADJEISAH.pdf"><i class="ai ai-cv"></i></a>
                            </li>
                             <li>
                                <a href="mailto:madjeisah@zjnu.edu.cn"><i class="fa fa-envelope"></i></a>
                            </li>
                           
                            <!--       <li>
                  <a href="#"><i class="ai ai-google-scholar-square ai-2x"></i></a>
                </li> -->
                        </ul>
                    </div>
                    <div class="copyright-text">
                        <p>Copyright 2019 Reflux Design</p>
                    </div>
                </div>
            </div>
        </div>
        <section class="section about-me" data-section="section1">
            <div class="container">
                <div class="section-heading">
                    <h2>About Me</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            I am currently a Postdoctoral Research Fellow in Zhejiang Normal University, working with Prof. <a href="https://xinzhongzhu.github.io"> Xinzhong Zhu </a>. My research interest is broadly in Natural Language Processing (NLP), machine learning, and deep learning. Currently, I am working on machine learning for Graphs representation learning. Methodologies of interest include but are not limited to multi-modal learning, transfer learning, graph neural networks, transformers, feature engineering, self-supervised learning and contrastive learning. I also have an exemplary passion and motivation in multidisciplinary research at the intersection of computer vision and NLP.
                        </p>
                        <p>
                            Prior to that, I received my Ph.D degree form the School of Computer Science and Technology, Donghua University, under the supervision of  Prof. <a href="https://cst.dhu.edu.cn/2012/0220/c3132a25128/page.htm"> Guohua Liu </a>. In 2016, I obtained my MSC. from the School of Computer Science and Engineering, Lanzhou University.
                        </p>
                        <!---<p>
                            If you are interested in our work and want to join us, please do not hesitate to drop me an email, <a href="mailto:madjeisah@hotmail.com"> madjeisah(at)hotmail.com </a>, with your resume.
                        </p> -->
                    </div>
                </div>
                <div class="section-news">
                    <h2>News</h2>
                    <div class="line-dec"></div>
                    <div class="news">
                        <p> 

                            <li style="margin: 5px;">
                                <b>2022-11:</b> Our paper on Data Augmentation in Graph Neural Network has been accepted by <a href="https://www.sciencedirect.com/journal/computer-science-review">Computer Science Review</a>.
                            </li>

                        	<li style="margin: 5px;">
                                <b>2022-10:</b> I will serve as a publicity chair for <a href="https://www2023.thewebconf.org/">TheWebConf2023</a>.
                            </li>
                            
                            <li style="margin: 5px;">
                                <b>2022-10:</b> 1 paper on Graph Contrastive Multiview Learning: A Pre-Training Framework is submitted to <a href="https://www.sciencedirect.com/journal/knowledge-based-systems">Knowledge-Based Systems</a>.
                            </li>
                            
                            <li style="margin: 5px;">
                                <b>2022-10:</b> 1 paper "SCMvL: A Search-based Contrastive Multi-View Learning" is submitted to <a href="https://www2023.thewebconf.org/">TheWebConf2023</a>.
                            </li>
                            
                            <li style="margin: 5px;">
                                <b>2022-10:</b> Our paper "Cooperative Window based Edge Partitioning via Edge Subtraction and Postponement" is submitted to <a href="https://www.sciencedirect.com/journal/computers-and-electrical-engineering">Computers and Electrical Engineering</a>.
                            </li>
                            
                        	
                            
                            
                            
                           <!-- <li style="margin: 5px;">
                                <b>2022-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-03:</b> 3 paper is accepted by <a href="https://cvpr2022.thecvf.com/">CVPR'2022</a>, 1 oral.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> Our python package for causal discovery <a href="https://causal-learn.readthedocs.io/en/latest/">causal-learn</a> is released. Any feedback is welcome.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> I give a talk at <a href="https://www.epfl.ch/en/">EPFL</a> to introduce using causal inference for computer vision task <a href="https://chengy12.github.io/files/Talk of Causility in CV.pdf">[slides]</a>, thanks <a href="https://sites.google.com/view/yuejiangliu/home">Yuejiang</a> for the invitation!
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 1 paper on person re-identification and attention learning is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 3 paper on trajectory prediction and attention learning is accepted by <a href="http://iccv2021.thecvf.com/">ICCV'2021</a>.
                            </li> -->
                            
                            
<!--                             <li style="margin: 5px;">
                                <b>2021-03:</b> 1 paper on unintentional action localization is accepted by <a href="https://2021.ieeeicme.org/">ICME'2021</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-07:</b> 2 papers on person re-identification are accepted by <a href="https://eccv2020.eu/">ECCV'2020</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-06:</b> Our team pangpang (I and <a href="https://raoyongming.github.io/">Yongming</a>) won the 2nd place in Semi-Supervised Recognition Challenge at <a href="https://sites.google.com/view/fgvc7">FGVC7</a> (CVPR 2020).
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-05:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-07:</b> 2 papers are accepted by <a href="http://iccv2019.thecvf.com/">ICCV'2019</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li> -->
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!--         <section class="section my-services" data-section="section3">
        <div class="container">
          <div class="section-heading">
            <h2>What Iâ€™m good at?</h2>
            <div class="line-dec"></div>
            <span
              >Curabitur leo felis, rutrum vitae varius eu, malesuada a tortor.
              Vestibulum congue leo et tellus aliquam, eu viverra nulla semper.
              Nullam eu faucibus diam. Donec eget massa ante.</span
            >
          </div>
          <div class="row">
            <div class="col-md-6">
              <div class="service-item">
                <div class="first-service-icon service-icon"></div>
                <h4>HTML5 &amp; CSS3</h4>
                <p>
                  Phasellus non convallis dolor. Integer tempor hendrerit arcu
                  at bibendum. Sed ac ante non metus vehicula congue quis eget
                  eros.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="second-service-icon service-icon"></div>
                <h4>Creative Ideas</h4>
                <p>
                  Proin lacus massa, eleifend sed fermentum in, dignissim vel
                  metus. Nunc accumsan leo nec felis porttitor, ultricies
                  faucibus purus mollis.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="third-service-icon service-icon"></div>
                <h4>Easy Customize</h4>
                <p>
                  Integer suscipit condimentum aliquet. Nam quis risus metus.
                  Nullam faucibus quam eget arcu pretium tincidunt. Nam libero
                  dui.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="fourth-service-icon service-icon"></div>
                <h4>Admin Dashboard</h4>
                <p>
                  Vivamus et dui a massa venenatis fringilla. Proin lacus massa,
                  eleifend sed fermentum in, dignissim vel metus. Nunc accumsan
                  leo nec felis porttitor.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
       -->
        <!-- <section class="section my-research" data-section="section2">
            <div class="container">
                <div class="section-heading">
                    <h2>My Research</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            My research interests spans across Computer Vision and Machine Learning. For computer vision, I focus on developing methods to better understand human, which includes person re-identification, pedestrian trajectory prediction, and human action understanding. On the machine learning site, I am interested in causal-based learning such as causal inference and causal representation learning.
                        </p>
                    </div>
                    <div class="row">
                        <div class="isotope-wrapper">
                            <form class="isotope-toolbar">
                                <label><input type="radio" data-type="*" checked="" name="isotope-filter" />
                                    <span>All</span></label>
                                <label><input type="radio" data-type="reid" name="isotope-filter" />
                                    <span>Re-identification</span></label>
                                <label><input type="radio" data-type="trajectory" name="isotope-filter" />
                                    <span>Trajectory</span></label>
                                <label><input type="radio" data-type="action" name="isotope-filter" />
                                    <span>Action</span></label>
                                <label><input type="radio" data-type="causality" name="isotope-filter" />
                                    <span>Causality</span></label>
                            </form>
                            <div class="isotope-box">
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/mid.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</h4>
                                            <span>We propose a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion, in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321">
                                        <img src="assets/images/finediving.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</h4>
                                            <span>We construct a new fine-grained dataset for the explainable action quality assessment, named FineDiving, developed on diverse diving events with detailed annotations on action procedures.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/causal.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Human Trajectory Prediction via Counterfactual Analysis</h4>
                                            <span>We propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predicted trajectories and input clues and alleviate the negative effects brought by environment bias.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/model_dis.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Personalized Trajectory Prediction via Distribution Discrimination</h4>
                                            <span> We present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions in a self-supervised manner.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321">
                                        <img src="assets/images/CAL.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</h4>
                                            <span>We propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/apnet.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Person Re-identification via Attention Pyramid</h4>
                                            <span>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales and obtain superior performance on many person re-identification datasets.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/0648.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</h4>
                                            <span>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/framework_0645.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</h4>
                                            <span>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321">
                                        <img src="assets/images/icme21.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Label Aggregation for Unintentional Action Localization</h4>
                                            <span>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/1046.PNG" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Self-Critical Attention Learning for Person Re-Identification</h4>
                                            <span>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
        </section> -->
        <section class="section publications" data-section="section3">
            <div class="container">
                <div class="section-heading">
                    <h2>Publications</h2>
                    <div class="line-dec"></div>
                    <!--                 </div>
                <div class="row"> -->
                    <div class="left">
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/topology.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle> Towards Data Augmentation in Graph Neural Network: An Overview and Evaluation.</papertitle>
                                        <br>

                                        <strong>Michael Adjeisah</strong>, Xinzhong Zhu*, Huiying Xu, and Tewodros Alemu Ayall
                                        
                                        <br>
                                        <em> Computer Science Review (<strong>COSREV</strong>)</em>, 2022
                                        <br>
                                        <a href="https://authors.elsevier.com/tracking/article/details.do?aid=100527&jid=COSREV&surname=Adjeisah">[Tracking]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/peerj-cs-08-877-g004.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">

                                        <papertitle>ACR-SA: attention-based deep model through two-channel CNN and Bi-RNN for sentiment analysis.</papertitle>
                                        <br>
                                        M. Kamyab, G. Liu*, A. Rasool, and <strong>Michael Adjeisah</strong>
                                        <br>
                                        <em>PeerJ Computer Science (<strong>PeerJ</strong>), 2022 </em>
                                        <br>
                                        <a href="https://peerj.com/articles/cs-877/">[PDF]</a> <a href="https://github.com/abdul-rasool/ACR-SA-Main">[Code]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/peerj-cs-08-877-g001.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Attention-Based CNN and Bi-LSTM Model Based on TF-IDF and GloVe Word Embedding for Sentiment Analysis.</papertitle>
                                        <br>
                                        Marjan Kamyab,Guohua Liu, and <strong>Michael Adjeisah</strong>
                                        <br>
                                         <em>Journals of Applied Sciences</em>, 2021.
                                        <br>
                                        <a href="https://www.mdpi.com/2076-3417/11/23/11255">[PDF]</a> <a href="https://github.com/abdul-rasool/ACR-SA-Main">[Code]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                        
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/1759111.fig.002.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>A 3D-2D Convolutional Neural Network and Transfer Learning for Hyperspectral Image Classification</papertitle>
                                        <br>
                                        Douglas Omwenga Nyabuga, Jinling Song*, Guohua Liu,and <strong>Michael Adjeisah</strong>
                                        <br>
                                        <em>Computational Intelligence and Neuroscience, 2022
                                        <br>
                                        <a href="https://www.hindawi.com/journals/cin/2021/1759111/">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/6682385.fig.002.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Pseudotext Injection and Advance Filtering of Low-Resource Corpus for Neural Machine Translation.</papertitle>
                                        <br>
                                        <strong>Michael Adjeisah</strong>, Guohua Liu, Douglas Omwenga Nyabuga, Richard Nuetey Nortey, and Jinling Song*
                                        <br>
                                        <em>Computational Intelligence and Neuroscience, 2021
                                        <br>
                                        <a href="https://www.hindawi.com/journals/cin/2021/6682385/">[PDF]</a> <a href="https://github.com/Madjeisah/tw-parallel-lg-corpus">[Code]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/2022-11-25 17-00-46.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Twi Corpus: A Massively Twi-to-Handful Languages Parallel Bible Corpus.</papertitle>
                                        <br>

                                        <strong>Michael Adjeisah</strong>, Guohua Liu, Richard Nuetey Nortey, Jinling Song*, Khalid Odartey Lamptey, and Felix Nana Frimpong
                                        <br>
                                        <em> 2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (<strong>IISPA/BDCloud/SocialCom/SustainCom</strong>)</em>, 2020
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/document/9443970">[PDF]</a> <a href="https://github.com/Madjeisah/tw-parallel-lg-corpus">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/res_img.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>A novel respiration pattern biometric prediction system based on artificial neural network.</papertitle>
                                        <br>
                                        Rafiu King Raji, <strong>Michael Adjeisah</strong>, Xuhong Miao, Ailan Wan
                                        <br>
                                        <em>Sensor Review</em>, 2020
                                        <br>
                                        <a href="https://www.emerald.com/insight/content/doi/10.1108/SR-10-2019-0235/full/html">[PDF]</a> <a href="https://github.com/Madjeisah/respred">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                     <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/res_img" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Multi-Sensor Information Fusion and Machine Learning for High Accuracy Rate of Mechanical Pedometer in Human Activity Recognition.</papertitle>
                                        <br>

                                        <strong>Michael Adjeisah</strong>, Guohua Liu, Douglas Omwenga Nyabuga, and Richard Nuetey Nortey
                                        <br>
                                        <em>  2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (<strong>IISPA/BDCloud/SocialCom/SustainCom</strong>)</em>, 2019
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/document/9047436">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/res_img" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Consonant Phoneme based Extreme Learning Machine (ELM) Recognition model for Foreign Accent Identification.</papertitle>
                                        <br>

                                        Kaleem Kashif, Yizhi Wu, Junjie Zhang, and <strong>Michael Adjeisah</strong>
                                        <br>
                                        <em>Proceedings of the World Symposium on Software Engineering (<strong>WSSE</strong>)</em>, 2021
                                        <br>
                                        <a href="https://dl.acm.org/doi/10.1145/3362125.3362130">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/b_chain.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Privacy Module for Distributed Electronic Health Records (EHRs) using the Blockchain.</papertitle>
                                        <br>
                                        Richard Nuetey Nortey, Li Yue, Promise Ricardo Agdedanu, and <strong> Michael Adjeisah</strong>
                                        <br>
                                        <em>2019 IEEE 4th International Conference on Big Data Analysis(<strong>ICBDA</strong>)</em>, 2019
                                        <br>
                                        <a href="files/Privacy_Module_for_Distributed_Electronic_Health_RecordsEHRs_Using_the_Blockchain.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr> 
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/526-BAI2019-229.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Blockchain-Enabled Privacy Security Module for Sharing Electronic Health Records (EHRs).</papertitle>
                                        <br>
                                        Li Yue, Richard Nuetey Nortey, <strong>Michael Adjeisah</strong>, Promise Ricardo Agbedanu, Xinyi Lui
                                        <br>
                                        <em>International Journal of Computer and Communication Engineering (<strong>IJCCE</strong>)</em>, 2019
                                        <br>
                                        <a href="http://www.ijcce.org/vol8/526-BAI2019-229.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/res_img.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>A New Approach for Tracking Human Body Movements by the Kinect Sensor</papertitle>
                                        <br>
                                        <strong>Michael Adjeisah</strong>, Zhao Chen, Guohua Liu, and Yang Yi
                                        <br>
                                        <em>IEEE 4th International Conference on Smart and Sustainable City (<strong>ICSSC</strong>)</em>, 2017
                                        <br>
                                        <a href="files/A_new_approach_for_tracking_human_body_movements_by_kinect_sensor .pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/filtering.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Joint Filtering: Enhancing Gesture and Mouse Movement in Microsoft Kinect Application</papertitle>
                                        <br>
                                        <strong>Michael Adjeisah</strong>, Yi Yang, and Lian Li
                                        <br>
                                        <em>IEEE 12th International Conference on Fuzzy Systems and Knowledge Discovery (<strong>FSKD</strong>)</em>, 2015
                                        <br>
                                        <a href="https://chengy12.github.io/files/1040_camera_ready_final.pdf">[PDF]</a> <a href="https://github.com/CHENGY12/DMML">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p></p>
                                    </td>
                                </tr>
                                <!--
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/1046.PNG" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Self-Critical Attention Learning for Person Re-Identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Chunze Lin, Liangliang Ren, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/1046_camera_ready_final.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</p>
                                    </td>
                                </tr>
                                -->
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </section>
        <section class="section teaching" data-section="section4">
            <div class="container">
                <div class="section-heading">
                    <h2>Teaching</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <p>
                        <li style="margin: 5px;">
                            <b>TA: </b>Big Data Technology and Application (04411), 2.5 Credit hours, Zhejiang Normal University.
                        </li>
                        <li style="margin: 5px;">
                            <b>TA: </b> Introduction to Deep Learning, 2.5 credit hours, Zhejiang Normal University.
                        </li>
                        <!--<li style="margin: 5px;">
                            <b>TA: </b> Machine Learning, Donghua University.
                        </li>-->
                    </p>
                </div>
            </div>
        </section>
        <!--         <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-services">
                    <h2>Services</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Competition Awards</heading>
                                    <p>
                                        <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Academic Services</heading>
                                    <p>
                                        <li style="margin: 5px;">
                                            <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICCV, ICML, NeurIPS, ICLR and so on.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Journal Reviewer:</b> TIP, TMM, TCSVT and so on.
                                        </li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section> -->
        <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-heading">
                    <h2>Academic Activities</h2>
                    <div class="line-dec"></div>
                    <h3>Award(s)</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;"> <a href="files/Michael_Adj_award.pdf">Excellent International Graduate </a> of Donghua University in the year, 2021</li>
                        </p>
                    </div>
                    <h3>Academic Services</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;">
                                <b>Publicity Chairs:</b> for TheWebConf, 2023.
                            </li>
                            <!--<li style="margin: 5px;">
                                <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                            </li> -->
                            <li style="margin: 5px;">
                                <b>Conference Reviewer / Program Committee Member:</b> AAAI, ACL, CVPR, COLING, ICML, ICLR NeurIPS, and so on.
                            </li>
                            <li style="margin: 5px;">
                                <b>Journal Reviewer:</b> KBS, TALLIP, IJCV, and so on.
                            </li>
                        </p>
                    </div>
                </div>
            </div>
        </section>
    </div>
    <!-- Scripts -->
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/js/isotope.min.js"></script>
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/lightbox.js"></script>
    <script src="assets/js/custom.js"></script>
    <script>
    //according to loftblog tut
    $(".main-menu li:first").addClass("active");

    var showSection = function showSection(section, isAnimate) {
        var direction = section.replace(/#/, ""),
            reqSection = $(".section").filter(
                '[data-section="' + direction + '"]'
            ),
            reqSectionPos = reqSection.offset().top - 0;

        if (isAnimate) {
            $("body, html").animate({
                    scrollTop: reqSectionPos
                },
                800
            );
        } else {
            $("body, html").scrollTop(reqSectionPos);
        }
    };

    var checkSection = function checkSection() {
        $(".section").each(function() {
            var $this = $(this),
                topEdge = $this.offset().top - 80,
                bottomEdge = topEdge + $this.height(),
                wScroll = $(window).scrollTop();
            if (topEdge < wScroll && bottomEdge > wScroll) {
                var currentId = $this.data("section"),
                    reqLink = $("a").filter("[href*=\\#" + currentId + "]");
                reqLink
                    .closest("li")
                    .addClass("active")
                    .siblings()
                    .removeClass("active");
            }
        });
    };

    $(".main-menu").on("click", "a", function(e) {
        e.preventDefault();
        showSection($(this).attr("href"), true);
    });

    $(window).scroll(function() {
        checkSection();
    });
    </script>
</body>

</html>
